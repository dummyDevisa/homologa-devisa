import openai
import asyncio
import streamlit as st
from typing import AsyncGenerator

# Erro na API: Error code: 401 - {'error': {'message': "team not allowed to access model. This team can "
# "only access models=['DeepSeek-R1-Distill-Qwen-32B', 'Meta-Llama-3-2-3B-Instruct', 'DeepSeek-R1', 'DeepSeek-R1-Distill-Qwen-14B', "
# "'BAAI-bge-large-en-v1-5', 'Meta-Llama-3-1-8B-Instruct-FP8', 'DeepSeek-R1-Distill-Llama-70B', 'Meta-Llama-3-3-70B-Instruct', "
# "'Meta-Llama-4-Maverick-17B-128E-Instruct-FP8']. Tried to access Meta-Llama-3-8B-Instruct", 'type': 'team_model_access_denied', 
# 'param': 'model', 'code': '401'}}

options = ["DeepSeek-R1-Distill-Qwen-32B", "Meta-Llama-3-2-3B-Instruct", "DeepSeek-R1", "DeepSeek-R1-Distill-Qwen-14B", 
           "BAAI-bge-large-en-v1-5", "Meta-Llama-3-1-8B-Instruct-FP8", "DeepSeek-R1-Distill-Llama-70B", "Meta-Llama-3-3-70B-Instruct", 
           "Meta-Llama-4-Maverick-17B-128E-Instruct-FP8"]

selection = st.pills("Modelos", options, selection_mode="single", default="Meta-Llama-4-Maverick-17B-128E-Instruct-FP8")

# --- Configura칞칚o da API ---
# Tenta carregar a chave da API Akash primeiro
try:
    api_key = st.secrets['dany']['akash_api']
    base_url = "https://chatapi.akash.network/api/v1"
    # Modelo dispon칤vel na Akash (ajuste se necess치rio)
    # Meta-Llama-3-2-3B-Instruct
    # DeepSeek-R1-Distill-Qwen-32B (coment치rios e tools)
    # Meta-Llama-3-3-70B-Instruct
    MODEL_NAME = selection
    # MODEL_NAME = "gpt-3.5-turbo" # Use um modelo comum como fallback ou ajuste
    print(f"INFO: Usando API Akash com base_url: {base_url}")
except KeyError:
    # Se a chave Akash n칚o for encontrada, tenta a chave OpenAI padr칚o
    try:
        api_key = st.secrets['dany']['openai_api']
        base_url = None # Usa o padr칚o da OpenAI
        MODEL_NAME = "gpt-3.5-turbo" # Modelo padr칚o da OpenAI
        print("INFO: Chave Akash n칚o encontrada. Usando API OpenAI padr칚o.")
    except KeyError:
        st.error("Erro: Nenhuma chave de API encontrada em st.secrets. Configure 'akash_api' ou 'openai_api' em .streamlit/secrets.toml")
        st.stop()
        api_key = None # Para evitar erros posteriores se st.stop() falhar por algum motivo
        base_url = None
        MODEL_NAME = None

# Instancia o cliente Async OpenAI (s칩 precisa ser feito uma vez)
# Verifica se api_key foi definida antes de criar o cliente
if api_key:
    client = openai.AsyncOpenAI(
        api_key=api_key,
        base_url=base_url # Ser치 None se estiver usando a API OpenAI padr칚o
    )
else:
    # O st.stop() deveria ter encerrado, mas por seguran칞a:
    client = None
    st.error("Cliente OpenAI n칚o p칪de ser inicializado devido  falta de API key.")
    st.stop()


async def get_chat_response_async(messages: list) -> AsyncGenerator[str, None]:
    """
    Obt칠m a resposta do chatbot de forma ass칤ncrona e em streaming.

    Par칙metros:
    - messages: Lista de mensagens da conversa (hist칩rico).

    Retorna:
    - Um gerador ass칤ncrono que produz peda칞os (chunks) da resposta.
    """
    if not client or not MODEL_NAME:
         yield "[Erro: Cliente ou modelo n칚o configurado]"
         return

    try:
        # Cria a chamada para a API com streaming habilitado
        stream = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=0.7, # Ajuste conforme necess치rio
            max_tokens=1024, # Ajuste conforme necess치rio
            stream=True,
        )

        # Itera sobre os chunks recebidos da API
        async for chunk in stream:
            content = chunk.choices[0].delta.content
            if content is not None:
                yield content # Retorna cada peda칞o de texto

    except Exception as e:
        print(f"Erro na API: {e}") # Logar o erro no console para depura칞칚o
        yield f"[Erro ao comunicar com a API: {str(e)}]"

# --- Interface Streamlit ---

# st.set_page_config(page_title="Chatbot com Mem칩ria", layout="wide")
st.title("游빍 Chatbot com Mem칩ria (Sess칚o)")
st.caption(f"Usando modelo: {MODEL_NAME} via {'Akash' if base_url else 'OpenAI'}")

# Inicializa o hist칩rico da conversa na session state se n칚o existir
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ol치! Como posso te ajudar hoje?"}]

# Exibe as mensagens do hist칩rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Campo de entrada para o usu치rio
if prompt := st.chat_input("Digite sua mensagem..."):
    # Adiciona a mensagem do usu치rio ao hist칩rico e exibe
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Prepara para receber e exibir a resposta do assistente em streaming
    with st.chat_message("assistant"):
        # Usa st.write_stream para exibir os chunks da resposta
        # Passa a fun칞칚o geradora ass칤ncrona diretamente
        # 칄 crucial passar o hist칩rico COMPLETO para o modelo ter contexto
        response_stream = get_chat_response_async(st.session_state.messages)

        # st.write_stream lida com a execu칞칚o ass칤ncrona e exibe os chunks
        # Ele tamb칠m retorna a resposta completa quando o stream termina
        full_response = st.write_stream(response_stream)

    # Adiciona a resposta completa do assistente ao hist칩rico da sess칚o
    # Isso garante que a mem칩ria seja mantida para a pr칩xima intera칞칚o
    if full_response: # Garante que n칚o adicionamos uma resposta vazia ou de erro mal formatado
       st.session_state.messages.append({"role": "assistant", "content": full_response})

    # Nota: O Streamlit automaticamente re-executa o script ap칩s o st.chat_input,
    # o que redesenha a interface com a nova mensagem do usu치rio e a resposta do assistente.
    # O loop 'for message in st.session_state.messages:' no in칤cio garante que
    # todo o hist칩rico atualizado seja exibido a cada intera칞칚o.

# Para rodar: streamlit run app.py